package builder;

import model.ApiLog;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import util.ApiLogParser;

import java.io.File;
import java.io.FilenameFilter;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.ResultSet;
import java.sql.Statement;

/**
 * Created on 20/09/2017.
 */
public class LogToCsvIngestion {
    private static final Logger LOGGER = LoggerFactory.getLogger(LogToCsvIngestion.class);
    private static final String TABLE_NAME = "apilog";
    private static final String H2_CONNECTION = "jdbc:h2:tcp://localhost/~/h2test";


    /**
     * Constructor
     */
    public void LogToCsvBuilder() {}

    /**
     * Prints how to invoke the app
     */
    // TODO: allow an input-folder as parameter and read all the files
    public static void usage() {
        LOGGER.info("java -jar target/log2csv-ingestion-*-jar-with-dependencies.jar ${input-log-file} ${output-dir-name}");
    }

    /**
     * Main class
     * @param args log file name
     */
    public static void main(String[] args) throws Exception {
        // Create the Spark Context.
        SparkConf conf = new SparkConf().setAppName("LogToCsvIngestion");
        JavaSparkContext sc = new JavaSparkContext(conf);
        File inputFile, outputApiLogDir = null;

        LogToCsvIngestion logToCsv = new LogToCsvIngestion();

        // Check the syntax
        if (args.length < 2 || args.length > 2) {
            LOGGER.error("Log file name and output directory are mandatory but they weren't provided as input parameter.");
            usage();
            System.exit(-1);
        }

        inputFile = new File(args[0]);
        outputApiLogDir = new File(args[1]);

        // transform
        logToCsv.transformLogFileIntoCsv(sc, inputFile.getCanonicalPath(), outputApiLogDir);

        // load
        logToCsv.loadCsvIntoH2(outputApiLogDir, H2_CONNECTION , TABLE_NAME, "", "", ApiLogParser.LOG_FIELDS_SCHEMA);

        // Stop the Spark context
        sc.stop();
    }


    /**
     * Creates a CSV file from a log file
     * @param sc {@link JavaSparkContext} Java Spark Context
     * @param logFile File to parse
     * @param outputDirName Directory name to save the output CSV file
     */
    public void transformLogFileIntoCsv(JavaSparkContext sc, String logFile, File outputDirName) {
        JavaRDD<String> apiLogLines = sc.textFile(logFile);

        LOGGER.info("Converting input file log into a Spark RDD...");
        // Convert the log lines to ApiLog objects
        JavaRDD<ApiLog> accessLogs =
                apiLogLines.map(ApiLogParser::parseFromLogLine).cache();

        LOGGER.info("Saving CSV file in " + outputDirName.getAbsolutePath());
        accessLogs.saveAsTextFile(outputDirName.getAbsolutePath());
    }

    /**
     * Load a CSV file into a local H2 database
     * @param outputCsvDir Directory where are the files to load
     * @return Number of loaded rows
     * @throws Exception
     */
    public int loadCsvIntoH2(File outputCsvDir, String connection, String user, String password, String tableName,
                             String csvSchema) throws Exception {
        Connection conn = null;
        Statement stmt = null;
        int loadedRows = -1;
        boolean tableExists = false;

        Class.forName("org.h2.Driver");
        conn = DriverManager.getConnection(connection, user, password);
        stmt = conn.createStatement();

        for(File file : getOutputSparkFiles(outputCsvDir)) {
            if(!tableExists) {
                stmt.execute("CREATE TABLE " + tableName +
                        " AS select * FROM CSVREAD('" +
                        file.getAbsolutePath() + "', '" +
                        csvSchema + "', " +
                        "'charset=UTF-8 fieldSeparator=|'" +
                        ")");

                tableExists = true;
            } else {
                stmt.execute("INSERT INTO apilog SELECT * FROM CSVREAD('" + file.getAbsolutePath() + "')");
            }
        }

        ResultSet rs = stmt.executeQuery("SELECT count(1) c from apilog");

        while( rs.next() ) {
            loadedRows = rs.getInt("c");
            LOGGER.info("Loaded " + loadedRows + " rows.");
        }

        return loadedRows;
    }


    /**
     * Get the files generated by the Spark process
     * @param outputApiLogFile Directory where the Spark process writes
     * @return The list of generated files (we assume that the file prefix is part-*)
     */
    private File[] getOutputSparkFiles(File outputApiLogFile) {
        File dir = new File(outputApiLogFile.getAbsolutePath());
        File [] files = dir.listFiles(new FilenameFilter() {
            @Override
            public boolean accept(File dir, String name) {
                return name.startsWith("part-");
            }
        });

        return files;
    }
}
